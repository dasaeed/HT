{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.special import digamma, loggamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[.,!?;:\\'\\\"\\-]\", \" \", text)\n",
    "    words = text.split()\n",
    "\n",
    "    return [w.strip() for w in words if w.strip()]\n",
    "\n",
    "def create_BoW_representation(doc_words, vocab_dict):\n",
    "    word_counts = Counter(word for word in doc_words if word in vocab_dict)\n",
    "    BoW_entries = []\n",
    "    for word, count in word_counts.items():\n",
    "        word_idx = vocab_dict[word]\n",
    "        BoW_entries.append(f\"{word_idx}:{count}\")\n",
    "    BoW_entries.sort(key=lambda x: int(x.split(\":\")[0]))\n",
    "\n",
    "    return f\"{len(BoW_entries)} {\" \".join(BoW_entries)}\"\n",
    "\n",
    "def process_documents(input_file, vocab_file):\n",
    "    with open(vocab_file, \"r\") as f:\n",
    "        vocab = [line.strip() for line in f.readlines()]\n",
    "    vocab_dict = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "    with open(input_file, \"r\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    docs = re.findall(r\"<TEXT>\\n(.*?)\\n </TEXT\", content, re.DOTALL)\n",
    "    BoW_representations = []\n",
    "    for doc in tqdm(docs):\n",
    "        words = clean_text(doc)\n",
    "        BoW = create_BoW_representation(words, vocab_dict)\n",
    "        BoW_representations.append(BoW)\n",
    "\n",
    "    return BoW_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2246/2246 [00:00<00:00, 3679.84it/s]\n"
     ]
    }
   ],
   "source": [
    "AP_BoW = process_documents(\"ap.txt\", \"vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2246/2246 [00:00<00:00, 5833.06it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    with open(\"vocab.txt\", \"r\") as f:\n",
    "        raw_lines = f.readlines()\n",
    "\n",
    "    idx_to_words = [word.strip() for word in raw_lines]\n",
    "    V = len(idx_to_words)\n",
    "\n",
    "    with open(\"ap_bow.txt\", \"r\") as f:\n",
    "        raw_lines = f.readlines()\n",
    "        N = len(raw_lines)\n",
    "        \n",
    "    articles = np.zeros((N, V))\n",
    "    nonzero_idxs = []\n",
    "\n",
    "    for i in tqdm(range(N)):\n",
    "        split = raw_lines[i].split(\" \")\n",
    "        n_words = int(split[0])\n",
    "        split = split[1:]\n",
    "\n",
    "        article = np.zeros((V,))\n",
    "        nonzero_idx = []\n",
    "\n",
    "        for bow in split:\n",
    "            bow = bow.strip()\n",
    "            word_idx, count = bow.split(\":\")\n",
    "            nonzero_idx.append(int(word_idx))\n",
    "            article[int(word_idx)] = count\n",
    "\n",
    "        try:\n",
    "            assert(len(nonzero_idx) == n_words)\n",
    "        except:\n",
    "            raise AssertionError(f\"{len(nonzero_idx)}, {n_words}\")\n",
    "\n",
    "        articles[i] = article\n",
    "        nonzero_idxs.append(sorted(nonzero_idx))\n",
    "    \n",
    "    return idx_to_words, articles, nonzero_idxs\n",
    "\n",
    "def init_variational_params(articles, K):\n",
    "    N, V = articles.shape\n",
    "    LAMBDA = np.random.uniform(low=0.01, high=1.0, size=(K, V))\n",
    "    GAMMA = np.ones((N, K))\n",
    "    PHI = []\n",
    "    for article in articles:\n",
    "        n_words = np.sum((article > 0).astype(\"int32\"))\n",
    "        article_PHI = np.ones((n_words, K))\n",
    "        article_PHI = article_PHI / K\n",
    "\n",
    "        PHI.append(article_PHI)\n",
    "    return LAMBDA, GAMMA, PHI\n",
    "\n",
    "idx_to_words, articles, nonzero_idxs = load_data()\n",
    "K = 30\n",
    "V = len(idx_to_words)\n",
    "ETA = 1 / V\n",
    "ALPHA = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ELBO(LAMBDA, GAMMA, PHI, articles, nonzero_idxs, K):\n",
    "    ELBO = 0\n",
    "    N, V = articles.shape\n",
    "\n",
    "    E_log_p_beta = 0\n",
    "    for k in range(K):\n",
    "        E_log_p_beta += (ETA-1) * np.sum(digamma(LAMBDA[k]) - digamma(np.sum(LAMBDA[k])))\n",
    "    ELBO += E_log_p_beta\n",
    "\n",
    "    E_log_p_theta = 0\n",
    "    for i in range(N):\n",
    "        E_log_p_theta += (ALPHA-1) * np.sum(digamma(GAMMA[i]) - digamma(np.sum(GAMMA[i])))\n",
    "    ELBO += E_log_p_theta\n",
    "\n",
    "    E_log_p_xz = 0\n",
    "    for i in range(N):\n",
    "        article = articles[i]\n",
    "        nonzero_idx = nonzero_idxs[i]\n",
    "        corr_idx = 0\n",
    "        for idx in nonzero_idx:\n",
    "            E_log_p_xz += article[idx] * np.sum(PHI[i][corr_idx] * (digamma(GAMMA[i]) - digamma(np.sum(GAMMA[i]))))\n",
    "            E_log_p_xz += article[idx] * np.sum(PHI[i][corr_idx] * (digamma(LAMBDA[:,idx]) - digamma(np.sum(LAMBDA, axis=1))))\n",
    "            corr_idx += 1\n",
    "\n",
    "        assert(corr_idx == len(nonzero_idx))\n",
    "    ELBO += E_log_p_xz\n",
    "\n",
    "    E_log_q_beta = 0\n",
    "    for k in range(K):\n",
    "        E_log_q_beta += -loggamma(np.sum(LAMBDA[k])) + np.sum(loggamma(LAMBDA[k]))\n",
    "        E_log_q_beta += -np.sum((LAMBDA[k]-1) * (digamma(LAMBDA[k]) - digamma(np.sum(LAMBDA[k]))))\n",
    "    ELBO += E_log_q_beta\n",
    "\n",
    "    E_log_q_theta = 0\n",
    "    for i in range(N):\n",
    "        E_log_q_theta += -loggamma(np.sum(GAMMA[i])) + np.sum(loggamma(GAMMA[i]))\n",
    "        E_log_q_theta += -np.sum((GAMMA[i]-1) * (digamma(GAMMA[i]) - digamma(np.sum(GAMMA[i]))))\n",
    "    ELBO += E_log_q_theta\n",
    "\n",
    "    E_log_q_z = 0\n",
    "    for i in range(N):\n",
    "        article = articles[i]\n",
    "        nonzero_idx = nonzero_idxs[i]\n",
    "        corr_idx = 0\n",
    "        for idx in nonzero_idx:\n",
    "            E_log_q_z += -article[idx] * np.sum(PHI[i][corr_idx] * np.log(PHI[i][corr_idx]))\n",
    "            corr_idx += 1\n",
    "\n",
    "        assert(corr_idx == len(nonzero_idx))\n",
    "    ELBO += E_log_q_z\n",
    "\n",
    "    return ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-5407631.986049961)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAMBDA_init, GAMMA_init, PHI_init = init_variational_params(articles, K)\n",
    "compute_ELBO(LAMBDA_init, GAMMA_init, PHI_init, articles, nonzero_idxs, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ht",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
