{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.special import digamma, loggamma\n",
    "\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocabulary(vocab_file: str) -> Dict[str, int]:\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    vocab = {}\n",
    "    with open(vocab_file, \"r\") as file:\n",
    "        for idx, word in enumerate(file):\n",
    "            if word not in stop_words:\n",
    "                vocab[word.strip()] = idx\n",
    "\n",
    "    return vocab\n",
    "\n",
    "def preprocess_text(text: str) -> List[str]:\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    text = text.lower()\n",
    "    words = re.findall(r\"\\b\\w+\\b\", text)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    return words\n",
    "\n",
    "def parse_documents(corpus_file: str) -> List[str]:\n",
    "    with open(corpus_file, \"r\") as file:\n",
    "        raw_text = file.read()\n",
    "    \n",
    "    pattern = r\"<TEXT>\\s*(.*?)\\s*</TEXT>\"\n",
    "    documents = re.findall(pattern, raw_text, re.DOTALL)\n",
    "\n",
    "    return documents\n",
    "\n",
    "def create_document_term_matrix(documents: List[str], vocab: Dict[str, int]) -> np.ndarray:\n",
    "    N = len(documents)\n",
    "    V = len(vocab)\n",
    "    doc_term_matrix = np.zeros((N, V), dtype=int)\n",
    "\n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        words = preprocess_text(doc)\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                vocab_idx = vocab[word]\n",
    "                doc_term_matrix[doc_idx, vocab_idx] += 1\n",
    "    \n",
    "    return doc_term_matrix\n",
    "\n",
    "documents = parse_documents(\"ap.txt\")\n",
    "vocab = load_vocabulary(\"vocab.txt\")\n",
    "dtm = create_document_term_matrix(documents=documents, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 18.0 MiB for an array with shape (10473, 225) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m LAMBDA \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mgamma(\u001b[38;5;241m100.\u001b[39m, \u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m100.\u001b[39m, (K, V))\n\u001b[0;32m      4\u001b[0m GAMMA \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((N, K))\n\u001b[1;32m----> 5\u001b[0m PHI \u001b[38;5;241m=\u001b[39m [\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m dtm]\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 18.0 MiB for an array with shape (10473, 225) and data type float64"
     ]
    }
   ],
   "source": [
    "K = 225\n",
    "N, V = dtm.shape\n",
    "LAMBDA = np.random.gamma(100., 1./100., (K, V))\n",
    "GAMMA = np.ones((N, K))\n",
    "PHI = [np.ones((len(doc), K)) / K for doc in dtm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(177)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = dtm[0]\n",
    "nonzero_idxs = np.sum(doc > 0)\n",
    "nonzero_idxs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ht",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
