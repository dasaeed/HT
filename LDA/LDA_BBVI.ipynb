{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from scipy.special import gammaln, digamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_LDA(K, V, N, M, eta0=0.1, alpha0=0.5, rs_int=npr.randint(low=0, high=100)):\n",
    "    rs = npr.RandomState(rs_int)\n",
    "    beta = rs.dirichlet(np.full(V, eta0), size=K)\n",
    "    theta = rs.dirichlet(np.full(K, alpha0), size=N)\n",
    "    X = []\n",
    "    for i in range(N):\n",
    "        x_i = np.zeros(M[i], dtype=int)\n",
    "        for j in range(M[i]):\n",
    "            z_ij = rs.choice(K, p=theta[i]) + 1\n",
    "            x_ij = rs.choice(V, p=beta[z_ij-1]) + 1\n",
    "            x_i[j] = x_ij\n",
    "        X.append(x_i)\n",
    "    return X\n",
    "\n",
    "def get_unique_idxs(X):\n",
    "    N = len(X)\n",
    "    Ms = [len(x_i) for x_i in X]\n",
    "    unique_idxs = []\n",
    "\n",
    "    for i in range(N):\n",
    "        unique_idxs_i = np.zeros(Ms[i], dtype=int)\n",
    "        for j in range(Ms[i]):\n",
    "            unique_idxs_i[j] = X[i][j] - 1\n",
    "        unique_idxs_i = np.unique(np.sort(unique_idxs_i))\n",
    "        unique_idxs.append(unique_idxs_i)\n",
    "    return unique_idxs\n",
    "\n",
    "def get_num_occs(X, unique_idxs):\n",
    "    N = len(X)\n",
    "    num_occs = []\n",
    "\n",
    "    for i in range(N):\n",
    "        counts = np.zeros(len(unique_idxs[i]), dtype=int)\n",
    "        for j, val in enumerate(unique_idxs[i]):\n",
    "            counts[j] = np.sum(X[i].astype(float) == (val+1))\n",
    "        num_occs.append(counts)\n",
    "    return num_occs\n",
    "\n",
    "def init_variational_params(X, K, V, rs_int=npr.randint(low=0, high=100)):\n",
    "    rs = npr.RandomState(rs_int)\n",
    "    N = len(X)\n",
    "    Ms = np.array([len(x_i) for x_i in X])\n",
    "    lambd = rs.uniform(low=0.01, high=1.0, size=(K, V))\n",
    "    # lambd = np.full((K, V), 1.0)\n",
    "    gamma = np.ones((N, K))\n",
    "    phi = []\n",
    "    for M_i in Ms:\n",
    "        phi_i = np.ones((M_i, K))\n",
    "        phi_i = phi_i / K\n",
    "        phi.append(phi_i)\n",
    "    return lambd, gamma, phi\n",
    "\n",
    "def log_dir(x, alpha):\n",
    "    return gammaln(np.sum(alpha)) - np.sum(gammaln(alpha)) + np.sum((alpha-1) * np.log(x + 1e-10))\n",
    "\n",
    "def compute_ELBO(lambd, gamma, phi, unique_idxs, num_occs):\n",
    "    N = gamma.shape[0]\n",
    "\n",
    "    E_log_p_beta = np.sum((eta0-1) * (digamma(lambd) - digamma(np.sum(lambd, axis=1, keepdims=True))))\n",
    "    E_log_p_theta = np.sum((alpha0-1) * (digamma(gamma) - digamma(np.sum(gamma, axis=1, keepdims=True))))\n",
    "    E_log_q_beta = np.sum(-gammaln(np.sum(lambd, axis=1)) + np.sum(gammaln(lambd), axis=1) \\\n",
    "            - np.sum((lambd - 1) * (digamma(lambd) - digamma(np.sum(lambd, axis=1, keepdims=True))), axis=1))\n",
    "    E_log_q_theta = np.sum(-gammaln(np.sum(gamma, axis=1)) + np.sum(gammaln(gamma), axis=1) \\\n",
    "            - np.sum((gamma - 1) * (digamma(gamma) - digamma(np.sum(gamma, axis=1, keepdims=True))), axis=1))\n",
    "    \n",
    "    E_log_p_x_z = 0.0\n",
    "    for i in range(N):\n",
    "        unique_idx = unique_idxs[i]\n",
    "        counts = num_occs[i]\n",
    "        j = 0\n",
    "        for idx in unique_idx:\n",
    "            E_log_p_x_z += counts[j] * np.sum(phi[i][j] * (digamma(gamma[i])-digamma(np.sum(gamma[i])))) \\\n",
    "                + counts[j] * np.sum(phi[i][j] * (digamma(lambd[:, idx])-digamma(np.sum(lambd, axis=1))))\n",
    "            j += 1\n",
    "\n",
    "    E_log_q_z = 0.0\n",
    "    for i in range(N):\n",
    "        unique_idx = unique_idxs[i]\n",
    "        counts = num_occs[i]\n",
    "        j = 0\n",
    "        for idx in unique_idx:\n",
    "            E_log_q_z += -np.sum(phi[i][j] * np.log(phi[i][j]))\n",
    "            j += 1\n",
    "    return E_log_p_beta + E_log_p_theta + E_log_q_beta + E_log_q_theta + E_log_p_x_z + E_log_q_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_variational_params(var_params):\n",
    "    lambd, gamma, phi = var_params\n",
    "    K, V = lambd.shape\n",
    "    N = gamma.shape[0]\n",
    "    Ms = np.array([len(phi[i]) for i in range(len(phi))])\n",
    "    \n",
    "    beta = np.zeros((K, V))\n",
    "    for k in range(K):\n",
    "        # beta[k] = npr.dirichlet(lambd[k])\n",
    "        beta[k] = npr.dirichlet(lambd[k]) + 1e-10\n",
    "        beta[k] /= beta[k].sum()\n",
    "    \n",
    "    theta = np.zeros((N, K))\n",
    "    for i in range(N):\n",
    "        # theta[i] = npr.dirichlet(gamma[i])\n",
    "        theta[i] = npr.dirichlet(gamma[i]) + 1e-10\n",
    "        theta[i] /= theta[i].sum()\n",
    "\n",
    "    z = []\n",
    "    for i in range(N):\n",
    "        z_i = np.zeros(Ms[i], dtype=int)\n",
    "        for j in range(Ms[i]):\n",
    "            z_i[j] = npr.choice(K, p=phi[i][j]) + 1\n",
    "        z.append(z_i)\n",
    "    \n",
    "    return beta, theta, z\n",
    "\n",
    "def log_variational_dist(latent_params, var_params):\n",
    "    lambd, gamma, phi = var_params\n",
    "    beta, theta, z = latent_params\n",
    "    K = lambd.shape[0]\n",
    "    N = gamma.shape[0]\n",
    "    Ms = np.array([len(phi[i]) for i in range(len(phi))])\n",
    "\n",
    "    log_q = 0.0\n",
    "    for k in range(K):\n",
    "        log_q += log_dir(beta[k], lambd[k])\n",
    "\n",
    "    for i in range(N):\n",
    "        log_q += log_dir(theta[i], gamma[i])\n",
    "        for j in range(Ms[i]):\n",
    "            for k in range(K):\n",
    "                log_q += float(z[i][j]-1 == k) * np.log(phi[i][j, k] + eps)\n",
    "    return log_q\n",
    "\n",
    "def log_joint_prob(latent_params, X):\n",
    "    beta, theta, z = latent_params\n",
    "    K, V = beta.shape\n",
    "    N = theta.shape[0]\n",
    "    Ms = np.array([len(z[i]) for i in range(len(z))])\n",
    "\n",
    "    log_p = 0.0\n",
    "    for k in range(K):\n",
    "        log_p += log_dir(beta[k], np.full(V, eta0))\n",
    "\n",
    "    for i in range(N):\n",
    "        log_p += log_dir(theta[i], np.full(K, alpha0))\n",
    "        for j in range(Ms[i]):\n",
    "            z_ij, x_ij = z[i][j], X[i][j]\n",
    "            log_p += np.log(theta[i, z_ij-1])\n",
    "            log_p += np.log(beta[z_ij-1, x_ij-1])\n",
    "    return log_p\n",
    "\n",
    "def score_variational_dist(latent_params, var_params):\n",
    "    beta, theta, z = latent_params\n",
    "    lambd, gamma, phi = var_params\n",
    "    K, V = lambd.shape\n",
    "    N = gamma.shape[0]\n",
    "    Ms = np.array([len(phi[i]) for i in range(len(phi))])\n",
    "    grad_lambda = np.zeros_like(lambd)\n",
    "    grad_gamma = np.zeros_like(gamma)\n",
    "    grad_phi = [np.zeros_like(phi_i) for phi_i in phi]\n",
    "\n",
    "    for k in range(K):\n",
    "        grad_lambda_k = np.zeros(V)\n",
    "        for v in range(V):\n",
    "            grad_lambda_k[v] = digamma(np.sum(lambd[k])) - digamma(lambd[k, v]) + np.log(beta[k, v] + 1e-10)\n",
    "        grad_lambda[k] = grad_lambda_k\n",
    "    \n",
    "    for i in range(N):\n",
    "        grad_gamma_i = np.zeros(K)\n",
    "        for k in range(K):\n",
    "            grad_gamma_i[k] = digamma(np.sum(gamma[i])) - digamma(gamma[i, k]) + np.log(theta[i, k] + 1e-10)\n",
    "        grad_gamma[i] = grad_gamma_i\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(Ms[i]):\n",
    "            grad_phi_ij = np.zeros(K)\n",
    "            for k in range(K):\n",
    "                grad_phi_ij[k] = float(z[i][j]-1 == k) / phi[i][j, k]\n",
    "            grad_phi[i][j] = grad_phi_ij\n",
    "    return grad_lambda, grad_gamma, grad_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5928.181058236594\n",
      "-8857.335788067563\n",
      "-1662.3702913520228\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "alpha <= 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1275], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m grad_phi \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mzeros_like(phi_i) \u001b[38;5;28;01mfor\u001b[39;00m phi_i \u001b[38;5;129;01min\u001b[39;00m phi]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(S):\n\u001b[1;32m---> 27\u001b[0m     beta_s, theta_s, z_s \u001b[38;5;241m=\u001b[39m \u001b[43msample_variational_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlambd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     grad_lambda_s, grad_gamma_s, grad_phi_s \u001b[38;5;241m=\u001b[39m score_variational_dist((beta_s, theta_s, z_s), (lambd, gamma, phi))\n\u001b[0;32m     29\u001b[0m     log_p, log_q \u001b[38;5;241m=\u001b[39m log_joint_prob((beta_s, theta_s, z_s), X), log_variational_dist((beta_s, theta_s, z_s), (lambd, gamma, phi))\n",
      "Cell \u001b[1;32mIn[1250], line 10\u001b[0m, in \u001b[0;36msample_variational_params\u001b[1;34m(var_params)\u001b[0m\n\u001b[0;32m      7\u001b[0m beta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((K, V))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(K):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# beta[k] = npr.dirichlet(lambd[k])\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     beta[k] \u001b[38;5;241m=\u001b[39m \u001b[43mnpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirichlet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlambd\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-10\u001b[39m\n\u001b[0;32m     11\u001b[0m     beta[k] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m beta[k]\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m     13\u001b[0m theta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((N, K))\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:4543\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.dirichlet\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: alpha <= 0"
     ]
    }
   ],
   "source": [
    "rs = npr.RandomState(0)\n",
    "eta0 = 0.3\n",
    "alpha0 = 0.5\n",
    "K = 5\n",
    "V = 100\n",
    "N = 10\n",
    "M = npr.poisson(50, size=N)\n",
    "X = simulate_LDA(K, V, N, M, eta0, alpha0)\n",
    "unique_idxs = get_unique_idxs(X)\n",
    "num_occs = get_num_occs(X, unique_idxs)\n",
    "\n",
    "S = 20\n",
    "rho = 0.01\n",
    "eps = 1e-6\n",
    "lambd, gamma, phi = init_variational_params(X, K, V)\n",
    "G_lambda = np.zeros_like(lambd)\n",
    "G_gamma = np.zeros_like(gamma)\n",
    "G_phi = [np.zeros_like(phi_i) for phi_i in phi]\n",
    "\n",
    "print(compute_ELBO(lambd, gamma, phi, unique_idxs, num_occs))\n",
    "\n",
    "for t in range(3):\n",
    "    grad_lambda = np.zeros_like(lambd)\n",
    "    grad_gamma = np.zeros_like(gamma)\n",
    "    grad_phi = [np.zeros_like(phi_i) for phi_i in phi]\n",
    "    for s in range(S):\n",
    "        beta_s, theta_s, z_s = sample_variational_params((lambd, gamma, phi))\n",
    "        grad_lambda_s, grad_gamma_s, grad_phi_s = score_variational_dist((beta_s, theta_s, z_s), (lambd, gamma, phi))\n",
    "        log_p, log_q = log_joint_prob((beta_s, theta_s, z_s), X), log_variational_dist((beta_s, theta_s, z_s), (lambd, gamma, phi))\n",
    "        grad_lambda += grad_lambda_s * (log_p - log_q)\n",
    "        grad_gamma += grad_gamma_s * (log_p - log_q)\n",
    "        for i in range(len(grad_phi)):\n",
    "            grad_phi[i] += grad_phi_s[i] * (log_p - log_q)\n",
    "    grad_lambda /= S\n",
    "    grad_gamma /= S\n",
    "    grad_phi = [g_phi / S for g_phi in grad_phi]\n",
    "\n",
    "    G_lambda += np.square(grad_lambda)\n",
    "    G_gamma += np.square(grad_gamma)\n",
    "    G_phi = [G_p + np.square(g_p) for G_p, g_p in zip(G_phi, grad_phi)]\n",
    "\n",
    "    rho_lambda = rho / (np.sqrt(G_lambda) + eps)\n",
    "    rho_gamma = rho / (np.sqrt(G_gamma) + eps)\n",
    "    rho_phi = [rho / (np.sqrt(G_p) + eps) for G_p in G_phi]\n",
    "\n",
    "    lambd += rho_lambda * grad_lambda\n",
    "    # lambd = np.maximum(lambd, 1e-3)\n",
    "    gamma += rho_gamma * grad_gamma\n",
    "    # gamma = np.maximum(gamma, 1e-3)\n",
    "    for i in range(len(phi)):\n",
    "        phi[i] += rho_phi[i] * grad_phi[i]\n",
    "        phi[i] = np.maximum(phi[i], 1e-10)\n",
    "        phi[i] /= phi[i].sum(axis=1, keepdims=True)\n",
    "            \n",
    "        # if t % 100 == 0:\n",
    "        #     print(t)\n",
    "    print(compute_ELBO(lambd, gamma, phi, unique_idxs, num_occs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72772895, 0.68986909, 0.5139746 , 0.81693573, 0.67666415,\n",
       "       0.72916291, 0.52684395, 0.71954486, 0.78033844, 0.74273193,\n",
       "       0.5948293 , 0.67831366, 0.60617041, 0.37333081, 0.9905991 ,\n",
       "       0.34404053, 0.54714105, 0.87253999, 0.3270894 , 0.84705663,\n",
       "       0.80041732, 0.4112345 , 0.47779821, 0.76437366, 0.98003957,\n",
       "       0.59814953, 0.66800309, 0.65138105, 0.84521121, 0.69373105,\n",
       "       0.55705901, 0.56451172, 0.40557593, 0.69829906, 0.64126996,\n",
       "       0.78974439, 0.42131593, 0.71449213, 0.85733011, 0.71171898,\n",
       "       0.59919093, 0.70407542, 0.66668948, 0.56677303, 0.52834288,\n",
       "       0.998471  , 0.87207133, 0.90712203, 0.71797699, 0.37034996,\n",
       "       0.6840777 , 0.84403024, 0.65780185, 0.93805543, 0.78395794,\n",
       "       0.61876346, 1.02362356, 0.92187373, 0.95010749, 0.67143199,\n",
       "       0.92023049, 0.68014147, 0.68260278, 0.96160331, 0.61899003,\n",
       "       0.62203863, 0.84426588, 0.84446863, 0.75051007, 0.84629898,\n",
       "       0.63250784, 0.77954966, 0.75507628, 0.71519343, 0.5847906 ,\n",
       "       0.74794242, 0.64556063, 0.5406613 , 0.37227337, 0.31101483,\n",
       "       0.90366619, 0.6904615 , 0.76596365, 0.71273474, 0.88767109,\n",
       "       0.63523562, 0.6411594 , 0.57761737, 0.72279556, 0.83528621,\n",
       "       0.50525405, 0.94916917, 0.94062822, 0.66223325, 0.82746224,\n",
       "       0.75078566, 0.96478768, 0.69170986, 0.60962017, 0.59946932])"
      ]
     },
     "execution_count": 1231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "scipy.stats.mode(lambd)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ht",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
